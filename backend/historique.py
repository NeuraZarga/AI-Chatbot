import json
from tokenizer import compter_tokens, optimiser_texte

# ğŸ”¹ Fichier oÃ¹ on stocke l'historique (simule une mÃ©moire)
HISTORIQUE_FILE = "data/historique.json"

# ğŸ”¹ Charge l'historique des conversations
def charger_historique():
    try:
        with open(HISTORIQUE_FILE, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        return []

# ğŸ”¹ Sauvegarde l'historique des conversations
def sauvegarder_historique(historique):
    with open(HISTORIQUE_FILE, "w") as f:
        json.dump(historique, f, indent=4)

# ğŸ”¹ Ajoute un message Ã  l'historique
def ajouter_message(role, message):
    historique = charger_historique()
    historique.append({"role": role, "content": message})

    # VÃ©rifie la taille de l'historique (on garde max 2000 tokens pour GPT-4 Turbo)
    tokens_total = sum(compter_tokens(msg["content"]) for msg in historique)
    while tokens_total > 2000:  # On rÃ©duit les anciens messages si on dÃ©passe
        historique[1]["content"] = optimiser_texte(historique[1]["content"])  # ğŸ”¹ Compression intelligente
        tokens_total = sum(compter_tokens(msg["content"]) for msg in historique)

    sauvegarder_historique(historique)

# ğŸ”¹ RÃ©cupÃ¨re l'historique formatÃ© pour OpenAI
def get_historique():
    return charger_historique()

# ğŸ”¹ TEST : VÃ©rifier que la mÃ©moire fonctionne bien
if __name__ == "__main__":
    ajouter_message("user", "Salut chatbot, comment vas-tu ?")
    ajouter_message("assistant", "Salut ! Je vais bien, merci. Comment puis-je t'aider ?")

    print("\nğŸ”¹ Historique actuel :")
    print(json.dumps(get_historique(), indent=4))
