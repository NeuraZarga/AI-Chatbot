import tiktoken
import openai
import json
import os

# RÃ©cupÃ©ration de la key openai
openai.api_key = os.getenv("OPENAI_API_KEY")

# Charger les paramÃ¨tres depuis un fichier de config
CONFIG_FILE = "config.json"

# Charger la config
def charger_config():
    try:
        with open(CONFIG_FILE, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return { "temperature" : 0.5} # TempÃ©rature par dÃ©faut
    
config = charger_config()
print(config["temperature"])

# ðŸ”¹ Initialisation du tokenizer OpenAI (GPT-4)
encoding = tiktoken.get_encoding("cl100k_base")

# ðŸ”¹ Fonction pour mesurer le nombre de tokens
def compter_tokens(texte):
    return len(encoding.encode(texte))

# ðŸ”¹ Fonction de rÃ©duction des tokens
def optimiser_texte(texte):
    try:
        response = openai.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "RÃ©sume ce texte de maniÃ¨re concise mais claire."},
                {"role": "user", "content": texte}
            ],
            max_tokens=30,
            temperature=config["temperature"]  # Ajustable depuis config.json
        )
        texte_reduit = response.choices[0].message.content
        
        # VÃ©rification si la phrase est coupÃ©e
        if texte_reduit[-1] not in ".!?":
            texte_reduit += "..."

        return texte_reduit
    except Exception as e:
        print("Erreur avec OpenAI :", e)
        return texte  # Retourne le texte original en cas d'erreur

# ðŸ”¹ TEST : VÃ©rifier que tout fonctionne bien
if __name__ == "__main__":
    texte_test = "Bonjour frÃ©rot, on optimise la tokenization pour notre chatbot afin de rÃ©duire les coÃ»ts !"
    
    print("\nðŸ”¹ Texte original :", texte_test)
    print(f"Nombre de tokens : {compter_tokens(texte_test)}")
    
    texte_optimise = optimiser_texte(texte_test)
    print("\nðŸ”¹ Texte optimisÃ© :", texte_optimise)
    print(f"Nombre de tokens aprÃ¨s optimisation : {compter_tokens(texte_optimise)}")
